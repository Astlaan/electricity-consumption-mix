{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from pandasgui import show\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "flow_pt_to_es = pd.read_pickle(\"../.data_cache/flow_pt_to_es.pkl.gz\")\n",
    "flow_es_to_pt = pd.read_pickle(\"../.data_cache/flow_es_to_pt.pkl.gz\")\n",
    "flow_fr_to_es = pd.read_pickle(\"../.data_cache/flow_fr_to_es.pkl.gz\")\n",
    "flow_es_to_fr = pd.read_pickle(\"../.data_cache/flow_es_to_fr.pkl.gz\")\n",
    "generation_pt = pd.read_pickle(\"../.data_cache/generation_pt.pkl.gz\")\n",
    "generation_es = pd.read_pickle(\"../.data_cache/generation_es.pkl.gz\")\n",
    "generation_fr = pd.read_pickle(\"../.data_cache/generation_fr.pkl.gz\")\n",
    "\n",
    "# Set index for flow dataframes\n",
    "flow_pt_to_es.set_index(\"start_time\", inplace=True)\n",
    "flow_es_to_pt.set_index(\"start_time\", inplace=True)\n",
    "flow_fr_to_es.set_index(\"start_time\", inplace=True)\n",
    "flow_es_to_fr.set_index(\"start_time\", inplace=True)\n",
    "generation_pt.set_index(\"start_time\", inplace=True)\n",
    "generation_es.set_index(\"start_time\", inplace=True)\n",
    "generation_fr.set_index(\"start_time\", inplace=True)\n",
    "\n",
    "\n",
    "# Show all dataframes using pandasgui\n",
    "# show(flow_pt_to_es)\n",
    "# show(flow_es_to_pt) \n",
    "# show(generation_pt)\n",
    "# show(generation_es)\n",
    "\n",
    "# Interpolate missing values for all dataframes\n",
    "import time\n",
    "\n",
    "# start_time = time.time()\n",
    "# flow_pt_to_es = flow_pt_to_es.interpolate(method='linear')\n",
    "# flow_es_to_pt = flow_es_to_pt.interpolate(method='linear') \n",
    "# flow_fr_to_es = flow_fr_to_es.interpolate(method='linear')\n",
    "# flow_es_to_fr = flow_es_to_fr.interpolate(method='linear')\n",
    "# generation_pt = generation_pt.interpolate(method='linear')\n",
    "# generation_es = generation_es.interpolate(method='linear')\n",
    "# generation_fr = generation_fr.interpolate(method='linear')\n",
    "# end_time = time.time()\n",
    "\n",
    "# start_time = time.time()\n",
    "# flow_pt_to_es = flow_pt_to_es.ffill()\n",
    "# flow_es_to_pt = flow_es_to_pt.ffill() \n",
    "# flow_fr_to_es = flow_fr_to_es.ffill()\n",
    "# flow_es_to_fr = flow_es_to_fr.ffill()\n",
    "# generation_pt = generation_pt.ffill()\n",
    "# generation_es = generation_es.ffill()\n",
    "# generation_fr = generation_fr.ffill()\n",
    "# end_time = time.time()\n",
    "\n",
    "# print(end_time-start_time)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[async_get_physical_flows] Start: 19:59:27, End: 19:59:27, took 0.04s: from: 10YES-REE------0 to 10YPT-REN------W\n",
      "[async_get_physical_flows] Start: 19:59:27, End: 19:59:27, took 0.04s: from: 10YES-REE------0 to 10YFR-RTE------C\n",
      "[async_get_physical_flows] Start: 19:59:27, End: 19:59:27, took 0.05s: from: 10YPT-REN------W to 10YES-REE------0\n",
      "[async_get_physical_flows] Start: 19:59:27, End: 19:59:27, took 0.06s: from: 10YFR-RTE------C to 10YES-REE------0\n",
      "[async_get_generation_data] Start: 19:59:27, End: 19:59:27, took 0.08s: country: 10YPT-REN------W\n",
      "[async_get_generation_data] Start: 19:59:27, End: 19:59:27, took 0.09s: country: 10YFR-RTE------C\n",
      "[async_get_generation_data] Start: 19:59:27, End: 19:59:27, took 0.10s: country: 10YES-REE------0\n",
      "[get_data] total duration: 0.10670232772827148s\n"
     ]
    }
   ],
   "source": [
    "from data_fetcher import ENTSOEDataFetcher\n",
    "import nest_asyncio\n",
    "from data_fetcher import SimpleInterval\n",
    "from datetime import datetime\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops (useful for Jupyter notebooks)\n",
    "fetcher = ENTSOEDataFetcher()\n",
    "\n",
    "data_request = SimpleInterval(datetime(2024, 1,1,0), datetime(2024, 1,1,1))\n",
    "data = fetcher.get_data(data_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>Power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78672</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2502.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      start_time   Power\n",
       "78672 2024-01-01  2502.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.flow_fr_to_es.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Power</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-10 00:00:00</th>\n",
       "      <td>1150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-10 01:00:00</th>\n",
       "      <td>1260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-10 02:00:00</th>\n",
       "      <td>1272.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-10 03:00:00</th>\n",
       "      <td>1258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-10 04:00:00</th>\n",
       "      <td>1243.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Power\n",
       "start_time                 \n",
       "2015-01-10 00:00:00  1150.0\n",
       "2015-01-10 01:00:00  1260.0\n",
       "2015-01-10 02:00:00  1272.0\n",
       "2015-01-10 03:00:00  1258.0\n",
       "2015-01-10 04:00:00  1243.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_fr_to_es.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change cache file format\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# flow_pt_to_es = pd.read_pickle(\"../.data_cache/flow_pt_to_es.pkl.gz\")\n",
    "# flow_es_to_pt = pd.read_pickle(\"../.data_cache/flow_es_to_pt.pkl.gz\")\n",
    "# generation_pt = pd.read_pickle(\"../.data_cache/generation_pt.pkl.gz\")\n",
    "# generation_es = pd.read_pickle(\"../.data_cache/generation_es.pkl.gz\")\n",
    "\n",
    "# # Save dataframes to pickle in .data_cache directory\n",
    "# flow_pt_to_es.to_pickle(\"../.data_cache/flow_pt_to_es.pkl.gz\", compression={'method': 'gzip', 'compresslevel': 1, \"mtime\": 0})\n",
    "# flow_es_to_pt.to_pickle(\"../.data_cache/flow_es_to_pt.pkl.gz\", compression={'method': 'gzip', 'compresslevel': 1, \"mtime\": 0}) \n",
    "# generation_pt.to_pickle(\"../.data_cache/generation_pt.pkl.gz\", compression={'method': 'gzip', 'compresslevel': 1, \"mtime\": 0})\n",
    "# generation_es.to_pickle(\"../.data_cache/generation_es.pkl.gz\", compression={'method': 'gzip', 'compresslevel': 1, \"mtime\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def check_continuous_hours(df, name):\n",
    "    min_date = df.index.min()\n",
    "    max_date = df.index.max()\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Start date min: {min_date}\")\n",
    "    print(f\"Start date max: {max_date}\")\n",
    "    \n",
    "    # Generate expected datetime range with hourly frequency\n",
    "    expected_dates = pd.date_range(start=min_date, end=max_date, freq='H')\n",
    "    \n",
    "    # Convert df dates to set for efficient lookup\n",
    "    actual_dates = set(df.index)\n",
    "    \n",
    "    # Find missing dates\n",
    "    missing_dates = set(expected_dates) - actual_dates\n",
    "    \n",
    "    if missing_dates:\n",
    "        print(f\"Missing {len(missing_dates)} hours:\")\n",
    "        for date in sorted(missing_dates)[:10]:  # Show first 10 missing dates\n",
    "            print(date)\n",
    "        if len(missing_dates) > 10:\n",
    "            print(\"...\")\n",
    "    else:\n",
    "        print(\"All hours present - continuous data\")\n",
    "\n",
    "# Check each dataframe\n",
    "check_continuous_hours(flow_pt_to_es, \"Flow PT to ES\")\n",
    "check_continuous_hours(flow_es_to_pt, \"Flow ES to PT\") \n",
    "check_continuous_hours(flow_es_to_fr, \"Flow ES to FR\") \n",
    "check_continuous_hours(flow_fr_to_es, \"Flow FR to ES\") \n",
    "check_continuous_hours(generation_pt, \"Generation PT\")\n",
    "check_continuous_hours(generation_es, \"Generation ES\")\n",
    "check_continuous_hours(generation_fr, \"Generation FR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nans(df, name):\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    # Get total number of NaN values\n",
    "    total_nans = df.isna().sum().sum()\n",
    "    \n",
    "    if total_nans > 0:\n",
    "        print(f\"Total NaN values: {total_nans}\")\n",
    "        # Show NaN counts by column\n",
    "        nan_counts = df.isna().sum()\n",
    "        nan_columns = nan_counts[nan_counts > 0]\n",
    "        print(\"\\nNaN counts by column:\")\n",
    "        for col, count in nan_columns.items():\n",
    "            print(f\"{col}: {count}/{len(df)}\")\n",
    "    else:\n",
    "        print(\"No NaN values found\")\n",
    "\n",
    "# Check each dataframe\n",
    "check_nans(flow_pt_to_es, \"Flow PT to ES\")\n",
    "check_nans(flow_es_to_pt, \"Flow ES to PT\")\n",
    "check_nans(flow_es_to_fr, \"Flow ES to FR\")\n",
    "check_nans(flow_fr_to_es, \"Flow FR to ES\")\n",
    "check_nans(generation_pt, \"Generation PT\") \n",
    "check_nans(generation_es, \"Generation ES\")\n",
    "check_nans(generation_fr, \"Generation FR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_nan_examples(df, name, n=100):\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    # Get columns with NaN values\n",
    "    nan_columns = df.columns[df.isna().any()].tolist()\n",
    "    \n",
    "    if not nan_columns:\n",
    "        print(\"No NaN values found\")\n",
    "        return\n",
    "        \n",
    "    print(\"\\nExample dates with NaN values:\")\n",
    "    for col in nan_columns:\n",
    "        nan_dates = df[df[col].isna()].index[:n]\n",
    "        if len(nan_dates) > 0:\n",
    "            print(f\"\\n{col}:\")\n",
    "            for date in nan_dates:\n",
    "                print(date)\n",
    "\n",
    "# Check each dataframe                \n",
    "# show_nan_examples(flow_pt_to_es, \"Flow PT to ES\")\n",
    "# show_nan_examples(flow_es_to_pt, \"Flow ES to PT\")\n",
    "# show_nan_examples(flow_es_to_fr, \"Flow ES to FR\")\n",
    "# show_nan_examples(flow_fr_to_es, \"Flow FR to ES\")\n",
    "# show_nan_examples(generation_pt, \"Generation PT\")\n",
    "# show_nan_examples(generation_es, \"Generation ES\")\n",
    "show_nan_examples(generation_fr, \"Generation FR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_fr[[\"B01\", \"B04\", \"B05\", \"B06\", \"B10\", \"B11\", \"B12\", \"B14\", \"B16\", \"B17\", \"B19\", \"B18\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_es[\"B10\"].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation_pt[\"B18\"].dropna()\n",
    "generation_pt[\"B18\"].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pt[\"B05\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "generation_pt[\"B18\"][-5:-1] = np.nan\n",
    "generation_pt[\"B18\"].interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if just one column without data -> it's fine\n",
    "if whole hour without data -> discard hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "flow_es_to_pt = pd.read_pickle(\"../.data_cache/flow_es_to_pt.pkl.gz\")\n",
    "flow_pt_to_es = pd.read_pickle(\"../.data_cache/flow_pt_to_es.pkl.gz\")\n",
    "flow_fr_to_es = pd.read_pickle(\"../.data_cache/flow_fr_to_es.pkl.gz\")\n",
    "flow_es_to_fr = pd.read_pickle(\"../.data_cache/flow_es_to_fr.pkl.gz\")\n",
    "\n",
    "flow_es_to_pt = flow_es_to_pt.fillna(0)\n",
    "flow_pt_to_es = flow_pt_to_es.fillna(0)\n",
    "flow_fr_to_es = flow_fr_to_es.fillna(0)\n",
    "flow_es_to_fr = flow_es_to_fr.fillna(0)\n",
    "\n",
    "print(\"es - pt:\", ((flow_es_to_pt[\"Power\"]*flow_pt_to_es[\"Power\"]) == 0).all()) \n",
    "print(\"es - fr:\", ((flow_es_to_fr[\"Power\"]*flow_fr_to_es[\"Power\"]) == 0).all())\n",
    "\n",
    "print(\"\\nCases where both flows are non-zero between ES-PT:\")\n",
    "mask_pt = (flow_es_to_pt[\"Power\"]*flow_pt_to_es[\"Power\"]) != 0\n",
    "if mask_pt.any():\n",
    "    conflicting_pt = pd.DataFrame({\n",
    "        'ES->PT': flow_es_to_pt.loc[mask_pt, \"Power\"],\n",
    "        'PT->ES': flow_pt_to_es.loc[mask_pt, \"Power\"]\n",
    "    })\n",
    "    print(conflicting_pt)\n",
    "else:\n",
    "    print(\"No conflicts found\")\n",
    "\n",
    "print(\"\\nCases where both flows are non-zero between ES-FR:\")    \n",
    "mask_fr = (flow_es_to_fr[\"Power\"]*flow_fr_to_es[\"Power\"]) != 0\n",
    "if mask_fr.any():\n",
    "    conflicting_fr = pd.DataFrame({\n",
    "        'ES->FR': flow_es_to_fr.loc[mask_fr, \"Power\"],\n",
    "        'FR->ES': flow_fr_to_es.loc[mask_fr, \"Power\"]\n",
    "    })\n",
    "    print(conflicting_fr)\n",
    "else:\n",
    "    print(\"No conflicts found\")\n",
    "\n",
    "print(\"\\nFraction of entries that don't verify the condition (both flows non-zero):\")\n",
    "print(\"ES-PT:\", (mask_pt).mean())\n",
    "print(\"ES-FR:\", (mask_fr).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSum of flows when both are non-zero:\")\n",
    "if mask_pt.any():\n",
    "    print(\"ES-PT:\", (conflicting_pt['ES->PT'] + conflicting_pt['PT->ES']).to_string())\n",
    "else:\n",
    "    print(\"ES-PT: No conflicts to sum\")\n",
    "    \n",
    "if mask_fr.any():\n",
    "    print(\"ES-FR:\", (conflicting_fr['ES->FR'] + conflicting_fr['FR->ES']).to_string()) \n",
    "else:\n",
    "    print(\"ES-FR: No conflicts to sum\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.tips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = px.data.gapminder().query(\"year == 2007\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei']\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "\n",
    "group_names = ['不依赖其他条目', '依赖其他条目']\n",
    "group_size = [24, 40]\n",
    "subgroup_names = ['', '1个', '2个', '3个', '4个', '5个', '9个', '13个']\n",
    "subgroup_size = [24, 15, 13, 5, 3, 2, 1, 1]\n",
    "\n",
    "a, b, c = [plt.cm.RdPu, plt.cm.GnBu, plt.cm.Greys]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis('equal')\n",
    "\n",
    "mypie, _ = ax.pie(subgroup_size, radius=1.3, labels=subgroup_names, labeldistance=0.85, colors=[c(0.0), b(0.7), b(0.6), b(0.5), b(0.4), b(0.3), b(0.2), b(0.1)])\n",
    "plt.setp(mypie, width=0.3, edgecolor='white')\n",
    "\n",
    "mypie2, _ = ax.pie(group_size, radius=1.3-0.3, labels=group_names, colors=[a(0.5), b(0.7)], labeldistance=0.6)\n",
    "plt.setp(mypie2, width=0.4, edgecolor='white')\n",
    "\n",
    "plt.margins(0, 0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sunburst.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.\n",
    "\n",
    "# Combine data into a single Sunburst trace\n",
    "labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "parents = [\"\", \"A\", \"A\", \"B\", \"B\", \"\", \"F\", \"F\"]\n",
    "values = [10, 20, 30, 40, 50, 15, 25, 35]\n",
    "\n",
    "fig = go.Figure(go.Sunburst(\n",
    "    labels=labels,\n",
    "    parents=parents,\n",
    "    values=values,\n",
    "    branchvalues=\"total\"\n",
    "))\n",
    "\n",
    "fig.update_layout(margin=dict(t=0, l=0, r=0, b=0))\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generation FR:\n",
    "Total NaN values: 117365\n",
    "\n",
    "NaN counts by column:\n",
    "B01: 31/86632\n",
    "B04: 31/86632\n",
    "B05: 4012/86632\n",
    "B06: 32/86632\n",
    "B10: 38909/86632\n",
    "B11: 34/86632\n",
    "B12: 246/86632\n",
    "B14: 32/86632\n",
    "B16: 22/86632\n",
    "B17: 31/86632\n",
    "B19: 32/86632\n",
    "B18: 73953/86632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation_fr[[\"B01\", \"B04\", \"B05\", \"B06\", \"B10\", \"B11\", \"B12\", \"B14\", \"B16\", \"B17\", \"B19\", \"B18\"]]\n",
    "generation_fr[\"B18\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pt[\"B12\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pt[\"B19\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pt[\"B19\"].sum()/generation_pt[\"B12\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pt[\"B12\"].nansum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pt[\"B19\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = generation_pt[\"B19\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test[-2:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sum(), np.nansum(test), test[0:3].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops (useful for Jupyter notebooks)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Append the 'src' directory to the system path to access custom modules\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Import necessary modules from your project\n",
    "from analyzer import analyze\n",
    "from data_fetcher import ENTSOEDataFetcher, SimpleInterval\n",
    "import utils  # Assuming utils is a module in the 'src' directory containing PSR_TYPE_MAPPING\n",
    "\n",
    "# Fetch the data using ENTSOEDataFetcher\n",
    "data_fetcher = ENTSOEDataFetcher()\n",
    "interval = SimpleInterval(datetime(2015, 1, 15, 0, 0), datetime(2024, 11, 29, 0, 0))\n",
    "data = data_fetcher.get_data(interval)\n",
    "\n",
    "# Analyze the data to get aggregated and contributions DataFrames\n",
    "aggregated, contributions = analyze(data)\n",
    "aggregated.columns = aggregated.columns.map(lambda x: utils.PSR_TYPE_MAPPING.get(x, x))\n",
    "\n",
    "# Normalize the DataFrame by dividing each element by the sum of its row and convert to percentages\n",
    "df_normalized = aggregated.div(aggregated.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Round the normalized percentages to two decimal places\n",
    "df_normalized = df_normalized.round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic summary statistics: Min, Max, Mean, Median\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Min': df_normalized.min(),\n",
    "    'Max': df_normalized.max(),\n",
    "    'Mean': df_normalized.mean(),\n",
    "    'Median': df_normalized.median()\n",
    "})\n",
    "\n",
    "# Initialize dictionaries to store the dates when Min and Max values occurred\n",
    "min_dates = {}\n",
    "max_dates = {}\n",
    "\n",
    "# Iterate over each column to find the dates of min and max values\n",
    "for column in df_normalized.columns:\n",
    "    min_value = df_normalized[column].min()\n",
    "    max_value = df_normalized[column].max()\n",
    "    \n",
    "    # Retrieve the timestamp where the min value occurs\n",
    "    min_date = df_normalized.index[df_normalized[column] == min_value][0]\n",
    "    \n",
    "    # Retrieve the timestamp where the max value occurs\n",
    "    max_date = df_normalized.index[df_normalized[column] == max_value][0]\n",
    "    \n",
    "    # Store the dates in the respective dictionaries\n",
    "    min_dates[column] = min_date\n",
    "    max_dates[column] = max_date\n",
    "\n",
    "# Add the Min and Max dates to the summary_stats DataFrame\n",
    "summary_stats['Min Date'] = pd.Series(min_dates)\n",
    "summary_stats['Max Date'] = pd.Series(max_dates)\n",
    "\n",
    "# Substitute the original index names with the mapped names from PSR_TYPE_MAPPING\n",
    "# If some mappings might be missing, use a lambda to retain original names\n",
    "# summary_stats.index = summary_stats.index.map(lambda x: utils.PSR_TYPE_MAPPING.get(x, x))\n",
    "\n",
    "# Reorder columns for better readability\n",
    "summary_stats = summary_stats[['Min', 'Min Date', 'Max', 'Max Date', 'Mean', 'Median']]\n",
    "\n",
    "# Adjust pandas display options to prevent wrapping and display all columns\n",
    "pd.set_option('display.max_columns', None)           # Display all columns\n",
    "pd.set_option('display.width', 1000)                 # Set a large display width\n",
    "pd.set_option('display.expand_frame_repr', False)    # Prevent wrapping to multiple lines\n",
    "\n",
    "# Option 1: Format percentage columns with '%' symbol by converting to strings\n",
    "summary_stats_formatted = summary_stats.copy()\n",
    "summary_stats_formatted[['Min', 'Max', 'Mean', 'Median']] = summary_stats_formatted[['Min', 'Max', 'Mean', 'Median']].applymap(lambda x: f\"{x:.2f}%\")\n",
    "\n",
    "# *** Enhanced Display Using Pandas Styler ***\n",
    "def highlight_min_max(s, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Highlight the min and max values in a series.\n",
    "    \"\"\"\n",
    "    return ['background-color: #FFDDC1' if v == min_val else \n",
    "            'background-color: #C1FFD7' if v == max_val else '' for v in s]\n",
    "\n",
    "# Create a Styler object\n",
    "styler = summary_stats.style\n",
    "\n",
    "# Apply highlighting to Min and Max columns\n",
    "for column in ['Min', 'Max']:\n",
    "    min_val = summary_stats[column].min()\n",
    "    max_val = summary_stats[column].max()\n",
    "    styler = styler.apply(lambda x: highlight_min_max(x, min_val, max_val), subset=[column])\n",
    "\n",
    "# Apply color gradients to percentage columns\n",
    "styler = styler.background_gradient(subset=['Min', 'Max', 'Mean', 'Median'], cmap='Blues')\n",
    "\n",
    "# Bold the header\n",
    "styler = styler.set_table_styles(\n",
    "    [{\n",
    "        'selector': 'th',\n",
    "        'props': [('font-size', '12pt'), ('text-align', 'center'), ('background-color', '#40466e'), ('color', 'white')]\n",
    "    },\n",
    "    {\n",
    "        'selector': 'td',\n",
    "        'props': [('padding', '5px'), ('text-align', 'center')]\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Alternate row colors for better readability\n",
    "# styler = styler.set_properties(**{'background-color': '#f9f9f9'}, subset=pd.IndexSlice[::2, :])\n",
    "\n",
    "# Format the date columns for better display\n",
    "styler = styler.format({\n",
    "    'Min Date': lambda v: v.strftime('%Y-%m-%d'),\n",
    "    'Max Date': lambda v: v.strftime('%Y-%m-%d'),\n",
    "    'Min': \"{:.2f}%\",\n",
    "    'Max': \"{:.2f}%\",\n",
    "    'Mean': \"{:.2f}%\",\n",
    "    'Median': \"{:.2f}%\"\n",
    "})\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styler\n",
    "\n",
    "# *** Optional: Export to HTML ***\n",
    "# To save the styled DataFrame as an HTML file (useful for reports)\n",
    "# with open('summary_stats.html', 'w') as f:\n",
    "#     f.write(styler.render())\n",
    "\n",
    "# Reset pandas display options to their default values if needed\n",
    "# pd.reset_option('display.max_columns')\n",
    "# pd.reset_option('display.width')\n",
    "# pd.reset_option('display.expand_frame_repr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic summary statistics: Min, Max, Mean, Median\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Min': df_normalized.min(),\n",
    "    'Max': df_normalized.max(),\n",
    "    'Mean': df_normalized.mean(),\n",
    "    'Median': df_normalized.median()\n",
    "})\n",
    "\n",
    "# Initialize dictionaries to store the dates when Min and Max values occurred\n",
    "min_dates = {}\n",
    "max_dates = {}\n",
    "\n",
    "# Iterate over each column to find the dates of min and max values\n",
    "for column in df_normalized.columns:\n",
    "    min_value = df_normalized[column].min()\n",
    "    max_value = df_normalized[column].max()\n",
    "    \n",
    "    # Retrieve the timestamp where the min value occurs\n",
    "    min_date = df_normalized.index[df_normalized[column] == min_value][0]\n",
    "    \n",
    "    # Retrieve the timestamp where the max value occurs\n",
    "    max_date = df_normalized.index[df_normalized[column] == max_value][0]\n",
    "    \n",
    "    # Store the dates in the respective dictionaries\n",
    "    min_dates[column] = min_date\n",
    "    max_dates[column] = max_date\n",
    "\n",
    "# Add the Min and Max dates to the summary_stats DataFrame\n",
    "summary_stats['Min Date'] = pd.Series(min_dates)\n",
    "summary_stats['Max Date'] = pd.Series(max_dates)\n",
    "\n",
    "# Substitute the original index names with the mapped names from PSR_TYPE_MAPPING\n",
    "# If some mappings might be missing, use a lambda to retain original names\n",
    "# summary_stats.index = summary_stats.index.map(lambda x: utils.PSR_TYPE_MAPPING.get(x, x))\n",
    "\n",
    "# Reorder columns for better readability\n",
    "summary_stats = summary_stats[['Min', 'Min Date', 'Max', 'Max Date', 'Mean', 'Median']]\n",
    "\n",
    "# Adjust pandas display options to prevent wrapping and display all columns\n",
    "pd.set_option('display.max_columns', None)           # Display all columns\n",
    "pd.set_option('display.width', 1000)                 # Set a large display width\n",
    "pd.set_option('display.expand_frame_repr', False)    # Prevent wrapping to multiple lines\n",
    "\n",
    "# Option 1: Format percentage columns with '%' symbol by converting to strings\n",
    "summary_stats_formatted = summary_stats.copy()\n",
    "summary_stats_formatted[['Min', 'Max', 'Mean', 'Median']] = summary_stats_formatted[['Min', 'Max', 'Mean', 'Median']].applymap(lambda x: f\"{x:.2f}%\")\n",
    "\n",
    "# *** Enhanced Display Using Pandas Styler ***\n",
    "def highlight_min_max(s, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Highlight the min and max values in a series.\n",
    "    \"\"\"\n",
    "    return ['background-color: #FFDDC1' if v == min_val else \n",
    "            'background-color: #C1FFD7' if v == max_val else '' for v in s]\n",
    "\n",
    "# Create a Styler object\n",
    "styler = summary_stats.style\n",
    "\n",
    "# Apply highlighting to Min and Max columns\n",
    "for column in ['Min', 'Max']:\n",
    "    min_val = summary_stats[column].min()\n",
    "    max_val = summary_stats[column].max()\n",
    "    styler = styler.apply(lambda x: highlight_min_max(x, min_val, max_val), subset=[column])\n",
    "\n",
    "# Apply color gradients to percentage columns\n",
    "styler = styler.background_gradient(subset=['Min', 'Max', 'Mean', 'Median'], cmap='Blues')\n",
    "\n",
    "# Bold the header\n",
    "styler = styler.set_table_styles(\n",
    "    [{\n",
    "        'selector': 'th',\n",
    "        'props': [('font-size', '12pt'), ('text-align', 'center'), ('background-color', '#40466e'), ('color', 'white')]\n",
    "    },\n",
    "    {\n",
    "        'selector': 'td',\n",
    "        'props': [('padding', '5px'), ('text-align', 'center')]\n",
    "    }]\n",
    ")\n",
    "\n",
    "# *** NEW STEP: Ensure Date Columns Have White Background ***\n",
    "styler = styler.set_properties(\n",
    "    subset=['Min Date', 'Max Date'],\n",
    "    **{'background-color': 'white'}\n",
    ")\n",
    "styler = styler.set_properties(\n",
    "    subset=['Min Date', 'Max Date'],\n",
    "    **{'color': 'gray'}\n",
    ")\n",
    "\n",
    "# Format the date columns for better display\n",
    "styler = styler.format({\n",
    "    'Min Date': lambda v: v.strftime('%Y-%m-%d'),\n",
    "    'Max Date': lambda v: v.strftime('%Y-%m-%d'),\n",
    "    'Min': \"{:.2f}%\",\n",
    "    'Max': \"{:.2f}%\",\n",
    "    'Mean': \"{:.2f}%\",\n",
    "    'Median': \"{:.2f}%\"\n",
    "})\n",
    "\n",
    "# Define zebra striping using CSS nth-child selectors\n",
    "styler = styler.set_table_styles([\n",
    "    {\n",
    "        'selector': 'tbody tr:nth-child(odd)',\n",
    "        'props': [('background-color', 'steelblue')]  # Light Blue\n",
    "    },\n",
    "    {\n",
    "        'selector': 'tbody tr:nth-child(even)',\n",
    "        'props': [('background-color', '#236ca8')]  # Dark Blue\n",
    "    }\n",
    "])\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styler\n",
    "\n",
    "# *** Optional: Export to HTML ***\n",
    "# To save the styled DataFrame as an HTML file (useful for reports)\n",
    "# with open('summary_stats.html', 'w') as f:\n",
    "#     f.write(styler.render())\n",
    "\n",
    "# Reset pandas display options to their default values if needed\n",
    "# pd.reset_option('display.max_columns')\n",
    "# pd.reset_option('display.width')\n",
    "# pd.reset_option('display.expand_frame_repr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Sample data (same as above)\n",
    "\n",
    "# Plotting with Plotly\n",
    "fig = px.line(df_normalized, x=df_normalized.index, y='Nuclear', title='Share of Nuclear in Portuguese electricity mix')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_normalized['MA15d'] = df_normalized['Nuclear'].rolling(window=15*24, min_periods=1).mean() #min_periods=1 to avoid NaN at the start\n",
    "df_normalized['MA30d'] = df_normalized['Nuclear'].rolling(window=30*24, min_periods=1).mean() #min_periods=1 to avoid NaN at the start\n",
    "df_normalized['MA180d'] = df_normalized['Nuclear'].rolling(window=180*24, min_periods=1).mean() #min_periods=1 to avoid NaN at the start\n",
    "df_normalized['MA365d'] = df_normalized['Nuclear'].rolling(window=365*24, min_periods=1).mean() #min_periods=1 to avoid NaN at the start\n",
    "\n",
    "\n",
    "# Plotting with Plotly\n",
    "fig = px.line(df_normalized, x=df_normalized.index, y=['Nuclear', 'MA15d', 'MA180d', 'MA365d'], title='Share of Nuclear in Portuguese electricity mix')\n",
    "# fig.update_traces(mode='lines+markers') #Adding markers for better visualization\n",
    "# fig.update_layout(legend_title_text = 'Legend')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Add rolling averages\n",
    "df_normalized['MA15d'] = df_normalized['Nuclear'].rolling(window=15*24, min_periods=1).mean()  # 15 days\n",
    "df_normalized['MA30d'] = df_normalized['Nuclear'].rolling(window=30*24, min_periods=1).mean()  # 30 days\n",
    "df_normalized['MA180d'] = df_normalized['Nuclear'].rolling(window=180*24, min_periods=1).mean()  # 6 months\n",
    "df_normalized['MA365d'] = df_normalized['Nuclear'].rolling(window=365*24, min_periods=1).mean()  # 1 year\n",
    "\n",
    "# Plotting with Plotly and customized traces\n",
    "fig = px.line(title='Share of Nuclear in Portuguese electricity mix')\n",
    "\n",
    "# Add traces with distinct styles\n",
    "fig.add_scatter(x=df_normalized.index, y=df_normalized['Nuclear'], mode='lines',\n",
    "                line=dict(color='blue', width=1), name='Nuclear', opacity=0.4)\n",
    "fig.add_scatter(x=df_normalized.index, y=df_normalized['MA15d'], mode='lines',\n",
    "                line=dict(color='red', dash='solid', width=2), name='15-day MA')\n",
    "# fig.add_scatter(x=df_normalized.index, y=df_normalized['MA30d'], mode='lines',\n",
    "#                 line=dict(color='green', dash='dot', width=2), name='30-day MA')\n",
    "fig.add_scatter(x=df_normalized.index, y=df_normalized['MA180d'], mode='lines',\n",
    "                line=dict(color='orange', dash='solid', width=2), name='180-day MA')\n",
    "fig.add_scatter(x=df_normalized.index, y=df_normalized['MA365d'], mode='lines',\n",
    "                line=dict(color='purple', dash='solid', width=2), name='365-day MA')\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Percentage (%)\",\n",
    "    legend_title=\"Legend\",\n",
    "    template=\"plotly_white\",\n",
    "    title_x=0.5,  # Center the title\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
