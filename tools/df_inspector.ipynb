{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_fetcher import ENTSOEDataFetcher\n",
    "import nest_asyncio\n",
    "from data_fetcher import SimpleInterval\n",
    "import analyzer\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "flow_pt_to_es: pd.DataFrame = pd.read_pickle(\"../.data_cache/flow_pt_to_es.pkl.gz\").interpolate()\n",
    "flow_es_to_pt: pd.DataFrame = pd.read_pickle(\"../.data_cache/flow_es_to_pt.pkl.gz\").interpolate()\n",
    "flow_fr_to_es: pd.DataFrame = pd.read_pickle(\"../.data_cache/flow_fr_to_es.pkl.gz\").interpolate()\n",
    "flow_es_to_fr: pd.DataFrame = pd.read_pickle(\"../.data_cache/flow_es_to_fr.pkl.gz\").interpolate()\n",
    "generation_pt: pd.DataFrame = pd.read_pickle(\"../.data_cache/generation_pt.pkl.gz\").interpolate()\n",
    "generation_es: pd.DataFrame = pd.read_pickle(\"../.data_cache/generation_es.pkl.gz\").interpolate()\n",
    "generation_fr: pd.DataFrame = pd.read_pickle(\"../.data_cache/generation_fr.pkl.gz\").interpolate()\n",
    "\n",
    "# Set index for flow dataframes\n",
    "flow_pt_to_es.set_index(\"start_time\", inplace=True)\n",
    "flow_es_to_pt.set_index(\"start_time\", inplace=True)\n",
    "flow_fr_to_es.set_index(\"start_time\", inplace=True)\n",
    "flow_es_to_fr.set_index(\"start_time\", inplace=True)\n",
    "generation_pt.set_index(\"start_time\", inplace=True)\n",
    "generation_es.set_index(\"start_time\", inplace=True)\n",
    "generation_fr.set_index(\"start_time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_fetcher import ENTSOEDataFetcher\n",
    "import nest_asyncio\n",
    "from data_fetcher import SimpleInterval\n",
    "from datetime import datetime\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops (useful for Jupyter notebooks)\n",
    "fetcher = ENTSOEDataFetcher()\n",
    "\n",
    "data_request = SimpleInterval(datetime(2024, 1,1,0), datetime(2024, 1,1,1))\n",
    "data = fetcher.get_data(data_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.flow_fr_to_es.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_fr_to_es.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change cache file format\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# flow_pt_to_es = pd.read_pickle(\"../.data_cache/flow_pt_to_es.pkl.gz\")\n",
    "# flow_es_to_pt = pd.read_pickle(\"../.data_cache/flow_es_to_pt.pkl.gz\")\n",
    "# generation_pt = pd.read_pickle(\"../.data_cache/generation_pt.pkl.gz\")\n",
    "# generation_es = pd.read_pickle(\"../.data_cache/generation_es.pkl.gz\")\n",
    "\n",
    "# # Save dataframes to pickle in .data_cache directory\n",
    "# flow_pt_to_es.to_pickle(\"../.data_cache/flow_pt_to_es.pkl.gz\", compression={'method': 'gzip', 'compresslevel': 1, \"mtime\": 0})\n",
    "# flow_es_to_pt.to_pickle(\"../.data_cache/flow_es_to_pt.pkl.gz\", compression={'method': 'gzip', 'compresslevel': 1, \"mtime\": 0}) \n",
    "# generation_pt.to_pickle(\"../.data_cache/generation_pt.pkl.gz\", compression={'method': 'gzip', 'compresslevel': 1, \"mtime\": 0})\n",
    "# generation_es.to_pickle(\"../.data_cache/generation_es.pkl.gz\", compression={'method': 'gzip', 'compresslevel': 1, \"mtime\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def check_continuous_hours(df, name):\n",
    "    min_date = df.index.min()\n",
    "    max_date = df.index.max()\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Start date min: {min_date}\")\n",
    "    print(f\"Start date max: {max_date}\")\n",
    "    \n",
    "    # Generate expected datetime range with hourly frequency\n",
    "    expected_dates = pd.date_range(start=min_date, end=max_date, freq='H')\n",
    "    \n",
    "    # Convert df dates to set for efficient lookup\n",
    "    actual_dates = set(df.index)\n",
    "    \n",
    "    # Find missing dates\n",
    "    missing_dates = set(expected_dates) - actual_dates\n",
    "    \n",
    "    if missing_dates:\n",
    "        print(f\"Missing {len(missing_dates)} hours:\")\n",
    "        for date in sorted(missing_dates)[:10]:  # Show first 10 missing dates\n",
    "            print(date)\n",
    "        if len(missing_dates) > 10:\n",
    "            print(\"...\")\n",
    "    else:\n",
    "        print(\"All hours present - continuous data\")\n",
    "\n",
    "# Check each dataframe\n",
    "check_continuous_hours(flow_pt_to_es, \"Flow PT to ES\")\n",
    "check_continuous_hours(flow_es_to_pt, \"Flow ES to PT\") \n",
    "check_continuous_hours(flow_es_to_fr, \"Flow ES to FR\") \n",
    "check_continuous_hours(flow_fr_to_es, \"Flow FR to ES\") \n",
    "check_continuous_hours(generation_pt, \"Generation PT\")\n",
    "check_continuous_hours(generation_es, \"Generation ES\")\n",
    "check_continuous_hours(generation_fr, \"Generation FR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nans(df, name):\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    # Get total number of NaN values\n",
    "    total_nans = df.isna().sum().sum()\n",
    "    \n",
    "    if total_nans > 0:\n",
    "        print(f\"Total NaN values: {total_nans}\")\n",
    "        # Show NaN counts by column\n",
    "        nan_counts = df.isna().sum()\n",
    "        nan_columns = nan_counts[nan_counts > 0]\n",
    "        print(\"\\nNaN counts by column:\")\n",
    "        for col, count in nan_columns.items():\n",
    "            print(f\"{col}: {count}/{len(df)}\")\n",
    "    else:\n",
    "        print(\"No NaN values found\")\n",
    "\n",
    "# Check each dataframe\n",
    "check_nans(flow_pt_to_es, \"Flow PT to ES\")\n",
    "check_nans(flow_es_to_pt, \"Flow ES to PT\")\n",
    "check_nans(flow_es_to_fr, \"Flow ES to FR\")\n",
    "check_nans(flow_fr_to_es, \"Flow FR to ES\")\n",
    "check_nans(generation_pt, \"Generation PT\") \n",
    "check_nans(generation_es, \"Generation ES\")\n",
    "check_nans(generation_fr, \"Generation FR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_nan_examples(df, name, n=100):\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    # Get columns with NaN values\n",
    "    nan_columns = df.columns[df.isna().any()].tolist()\n",
    "    \n",
    "    if not nan_columns:\n",
    "        print(\"No NaN values found\")\n",
    "        return\n",
    "        \n",
    "    print(\"\\nExample dates with NaN values:\")\n",
    "    for col in nan_columns:\n",
    "        nan_dates = df[df[col].isna()].index[:n]\n",
    "        if len(nan_dates) > 0:\n",
    "            print(f\"\\n{col}:\")\n",
    "            for date in nan_dates:\n",
    "                print(date)\n",
    "\n",
    "# Check each dataframe                \n",
    "# show_nan_examples(flow_pt_to_es, \"Flow PT to ES\")\n",
    "# show_nan_examples(flow_es_to_pt, \"Flow ES to PT\")\n",
    "# show_nan_examples(flow_es_to_fr, \"Flow ES to FR\")\n",
    "# show_nan_examples(flow_fr_to_es, \"Flow FR to ES\")\n",
    "# show_nan_examples(generation_pt, \"Generation PT\")\n",
    "# show_nan_examples(generation_es, \"Generation ES\")\n",
    "show_nan_examples(generation_fr, \"Generation FR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_fr[[\"B01\", \"B04\", \"B05\", \"B06\", \"B10\", \"B11\", \"B12\", \"B14\", \"B16\", \"B17\", \"B19\", \"B18\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_es[\"B10\"].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation_pt[\"B18\"].dropna()\n",
    "generation_pt[\"B18\"].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pt[\"B05\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "generation_pt[\"B18\"][-5:-1] = np.nan\n",
    "generation_pt[\"B18\"].interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if just one column without data -> it's fine\n",
    "if whole hour without data -> discard hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "flow_es_to_pt = pd.read_pickle(\"../.data_cache/flow_es_to_pt.pkl.gz\")\n",
    "flow_pt_to_es = pd.read_pickle(\"../.data_cache/flow_pt_to_es.pkl.gz\")\n",
    "flow_fr_to_es = pd.read_pickle(\"../.data_cache/flow_fr_to_es.pkl.gz\")\n",
    "flow_es_to_fr = pd.read_pickle(\"../.data_cache/flow_es_to_fr.pkl.gz\")\n",
    "\n",
    "flow_es_to_pt = flow_es_to_pt.fillna(0)\n",
    "flow_pt_to_es = flow_pt_to_es.fillna(0)\n",
    "flow_fr_to_es = flow_fr_to_es.fillna(0)\n",
    "flow_es_to_fr = flow_es_to_fr.fillna(0)\n",
    "\n",
    "print(\"es - pt:\", ((flow_es_to_pt[\"Power\"]*flow_pt_to_es[\"Power\"]) == 0).all()) \n",
    "print(\"es - fr:\", ((flow_es_to_fr[\"Power\"]*flow_fr_to_es[\"Power\"]) == 0).all())\n",
    "\n",
    "print(\"\\nCases where both flows are non-zero between ES-PT:\")\n",
    "mask_pt = (flow_es_to_pt[\"Power\"]*flow_pt_to_es[\"Power\"]) != 0\n",
    "if mask_pt.any():\n",
    "    conflicting_pt = pd.DataFrame({\n",
    "        'ES->PT': flow_es_to_pt.loc[mask_pt, \"Power\"],\n",
    "        'PT->ES': flow_pt_to_es.loc[mask_pt, \"Power\"]\n",
    "    })\n",
    "    print(conflicting_pt)\n",
    "else:\n",
    "    print(\"No conflicts found\")\n",
    "\n",
    "print(\"\\nCases where both flows are non-zero between ES-FR:\")    \n",
    "mask_fr = (flow_es_to_fr[\"Power\"]*flow_fr_to_es[\"Power\"]) != 0\n",
    "if mask_fr.any():\n",
    "    conflicting_fr = pd.DataFrame({\n",
    "        'ES->FR': flow_es_to_fr.loc[mask_fr, \"Power\"],\n",
    "        'FR->ES': flow_fr_to_es.loc[mask_fr, \"Power\"]\n",
    "    })\n",
    "    print(conflicting_fr)\n",
    "else:\n",
    "    print(\"No conflicts found\")\n",
    "\n",
    "print(\"\\nFraction of entries that don't verify the condition (both flows non-zero):\")\n",
    "print(\"ES-PT:\", (mask_pt).mean())\n",
    "print(\"ES-FR:\", (mask_fr).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSum of flows when both are non-zero:\")\n",
    "if mask_pt.any():\n",
    "    print(\"ES-PT:\", (conflicting_pt['ES->PT'] + conflicting_pt['PT->ES']).to_string())\n",
    "else:\n",
    "    print(\"ES-PT: No conflicts to sum\")\n",
    "    \n",
    "if mask_fr.any():\n",
    "    print(\"ES-FR:\", (conflicting_fr['ES->FR'] + conflicting_fr['FR->ES']).to_string()) \n",
    "else:\n",
    "    print(\"ES-FR: No conflicts to sum\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.tips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = px.data.gapminder().query(\"year == 2007\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei']\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "\n",
    "group_names = ['不依赖其他条目', '依赖其他条目']\n",
    "group_size = [24, 40]\n",
    "subgroup_names = ['', '1个', '2个', '3个', '4个', '5个', '9个', '13个']\n",
    "subgroup_size = [24, 15, 13, 5, 3, 2, 1, 1]\n",
    "\n",
    "a, b, c = [plt.cm.RdPu, plt.cm.GnBu, plt.cm.Greys]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis('equal')\n",
    "\n",
    "mypie, _ = ax.pie(subgroup_size, radius=1.3, labels=subgroup_names, labeldistance=0.85, colors=[c(0.0), b(0.7), b(0.6), b(0.5), b(0.4), b(0.3), b(0.2), b(0.1)])\n",
    "plt.setp(mypie, width=0.3, edgecolor='white')\n",
    "\n",
    "mypie2, _ = ax.pie(group_size, radius=1.3-0.3, labels=group_names, colors=[a(0.5), b(0.7)], labeldistance=0.6)\n",
    "plt.setp(mypie2, width=0.4, edgecolor='white')\n",
    "\n",
    "plt.margins(0, 0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sunburst.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.\n",
    "\n",
    "# Combine data into a single Sunburst trace\n",
    "labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "parents = [\"\", \"A\", \"A\", \"B\", \"B\", \"\", \"F\", \"F\"]\n",
    "values = [10, 20, 30, 40, 50, 15, 25, 35]\n",
    "\n",
    "fig = go.Figure(go.Sunburst(\n",
    "    labels=labels,\n",
    "    parents=parents,\n",
    "    values=values,\n",
    "    branchvalues=\"total\"\n",
    "))\n",
    "\n",
    "fig.update_layout(margin=dict(t=0, l=0, r=0, b=0))\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generation FR:\n",
    "Total NaN values: 117365\n",
    "\n",
    "NaN counts by column:\n",
    "B01: 31/86632\n",
    "B04: 31/86632\n",
    "B05: 4012/86632\n",
    "B06: 32/86632\n",
    "B10: 38909/86632\n",
    "B11: 34/86632\n",
    "B12: 246/86632\n",
    "B14: 32/86632\n",
    "B16: 22/86632\n",
    "B17: 31/86632\n",
    "B19: 32/86632\n",
    "B18: 73953/86632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops (useful for Jupyter notebooks)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Append the 'src' directory to the system path to access custom modules\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Import necessary modules from your project\n",
    "from analyzer import analyze\n",
    "from data_fetcher import ENTSOEDataFetcher, SimpleInterval\n",
    "import utils  # Assuming utils is a module in the 'src' directory containing PSR_TYPE_MAPPING\n",
    "\n",
    "# Fetch the data using ENTSOEDataFetcher\n",
    "data_fetcher = ENTSOEDataFetcher()\n",
    "interval = SimpleInterval(datetime(2015, 1, 15, 0, 0), datetime(2024, 11, 29, 0, 0))\n",
    "data = data_fetcher.get_data(interval)\n",
    "\n",
    "# Analyze the data to get aggregated and contributions DataFrames\n",
    "aggregated, contributions = analyze(data)\n",
    "aggregated.columns = aggregated.columns.map(lambda x: utils.PSR_TYPE_MAPPING.get(x, x))\n",
    "\n",
    "# Normalize the DataFrame by dividing each element by the sum of its row and convert to percentages\n",
    "df_normalized = aggregated.div(aggregated.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Round the normalized percentages to two decimal places\n",
    "df_normalized = df_normalized.round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic summary statistics: Min, Max, Mean, Median\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Min': df_normalized.min(),\n",
    "    'Max': df_normalized.max(),\n",
    "    'Mean': df_normalized.mean(),\n",
    "    'Median': df_normalized.median()\n",
    "})\n",
    "\n",
    "# Initialize dictionaries to store the dates when Min and Max values occurred\n",
    "min_dates = {}\n",
    "max_dates = {}\n",
    "\n",
    "# Iterate over each column to find the dates of min and max values\n",
    "for column in df_normalized.columns:\n",
    "    min_value = df_normalized[column].min()\n",
    "    max_value = df_normalized[column].max()\n",
    "    \n",
    "    # Retrieve the timestamp where the min value occurs\n",
    "    min_date = df_normalized.index[df_normalized[column] == min_value][0]\n",
    "    \n",
    "    # Retrieve the timestamp where the max value occurs\n",
    "    max_date = df_normalized.index[df_normalized[column] == max_value][0]\n",
    "    \n",
    "    # Store the dates in the respective dictionaries\n",
    "    min_dates[column] = min_date\n",
    "    max_dates[column] = max_date\n",
    "\n",
    "# Add the Min and Max dates to the summary_stats DataFrame\n",
    "summary_stats['Min Date'] = pd.Series(min_dates)\n",
    "summary_stats['Max Date'] = pd.Series(max_dates)\n",
    "\n",
    "# Substitute the original index names with the mapped names from PSR_TYPE_MAPPING\n",
    "# If some mappings might be missing, use a lambda to retain original names\n",
    "# summary_stats.index = summary_stats.index.map(lambda x: utils.PSR_TYPE_MAPPING.get(x, x))\n",
    "\n",
    "# Reorder columns for better readability\n",
    "summary_stats = summary_stats[['Min', 'Min Date', 'Max', 'Max Date', 'Mean', 'Median']]\n",
    "\n",
    "# Adjust pandas display options to prevent wrapping and display all columns\n",
    "pd.set_option('display.max_columns', None)           # Display all columns\n",
    "pd.set_option('display.width', 1000)                 # Set a large display width\n",
    "pd.set_option('display.expand_frame_repr', False)    # Prevent wrapping to multiple lines\n",
    "\n",
    "# Option 1: Format percentage columns with '%' symbol by converting to strings\n",
    "summary_stats_formatted = summary_stats.copy()\n",
    "summary_stats_formatted[['Min', 'Max', 'Mean', 'Median']] = summary_stats_formatted[['Min', 'Max', 'Mean', 'Median']].applymap(lambda x: f\"{x:.2f}%\")\n",
    "\n",
    "# *** Enhanced Display Using Pandas Styler ***\n",
    "def highlight_min_max(s, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Highlight the min and max values in a series.\n",
    "    \"\"\"\n",
    "    return ['background-color: #FFDDC1' if v == min_val else \n",
    "            'background-color: #C1FFD7' if v == max_val else '' for v in s]\n",
    "\n",
    "# Create a Styler object\n",
    "styler = summary_stats.style\n",
    "\n",
    "# Apply highlighting to Min and Max columns\n",
    "for column in ['Min', 'Max']:\n",
    "    min_val = summary_stats[column].min()\n",
    "    max_val = summary_stats[column].max()\n",
    "    styler = styler.apply(lambda x: highlight_min_max(x, min_val, max_val), subset=[column])\n",
    "\n",
    "# Apply color gradients to percentage columns\n",
    "styler = styler.background_gradient(subset=['Min', 'Max', 'Mean', 'Median'], cmap='Blues')\n",
    "\n",
    "# Bold the header\n",
    "styler = styler.set_table_styles(\n",
    "    [{\n",
    "        'selector': 'th',\n",
    "        'props': [('font-size', '12pt'), ('text-align', 'center'), ('background-color', '#40466e'), ('color', 'white')]\n",
    "    },\n",
    "    {\n",
    "        'selector': 'td',\n",
    "        'props': [('padding', '5px'), ('text-align', 'center')]\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Alternate row colors for better readability\n",
    "# styler = styler.set_properties(**{'background-color': '#f9f9f9'}, subset=pd.IndexSlice[::2, :])\n",
    "\n",
    "# Format the date columns for better display\n",
    "styler = styler.format({\n",
    "    'Min Date': lambda v: v.strftime('%Y-%m-%d'),\n",
    "    'Max Date': lambda v: v.strftime('%Y-%m-%d'),\n",
    "    'Min': \"{:.2f}%\",\n",
    "    'Max': \"{:.2f}%\",\n",
    "    'Mean': \"{:.2f}%\",\n",
    "    'Median': \"{:.2f}%\"\n",
    "})\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styler\n",
    "\n",
    "# *** Optional: Export to HTML ***\n",
    "# To save the styled DataFrame as an HTML file (useful for reports)\n",
    "# with open('summary_stats.html', 'w') as f:\n",
    "#     f.write(styler.render())\n",
    "\n",
    "# Reset pandas display options to their default values if needed\n",
    "# pd.reset_option('display.max_columns')\n",
    "# pd.reset_option('display.width')\n",
    "# pd.reset_option('display.expand_frame_repr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic summary statistics: Min, Max, Mean, Median\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Min': df_normalized.min(),\n",
    "    'Max': df_normalized.max(),\n",
    "    'Mean': df_normalized.mean(),\n",
    "    'Median': df_normalized.median()\n",
    "})\n",
    "\n",
    "# Initialize dictionaries to store the dates when Min and Max values occurred\n",
    "min_dates = {}\n",
    "max_dates = {}\n",
    "\n",
    "# Iterate over each column to find the dates of min and max values\n",
    "for column in df_normalized.columns:\n",
    "    min_value = df_normalized[column].min()\n",
    "    max_value = df_normalized[column].max()\n",
    "    \n",
    "    # Retrieve the timestamp where the min value occurs\n",
    "    min_date = df_normalized.index[df_normalized[column] == min_value][0]\n",
    "    \n",
    "    # Retrieve the timestamp where the max value occurs\n",
    "    max_date = df_normalized.index[df_normalized[column] == max_value][0]\n",
    "    \n",
    "    # Store the dates in the respective dictionaries\n",
    "    min_dates[column] = min_date\n",
    "    max_dates[column] = max_date\n",
    "\n",
    "# Add the Min and Max dates to the summary_stats DataFrame\n",
    "summary_stats['Min Date'] = pd.Series(min_dates)\n",
    "summary_stats['Max Date'] = pd.Series(max_dates)\n",
    "\n",
    "# Substitute the original index names with the mapped names from PSR_TYPE_MAPPING\n",
    "# If some mappings might be missing, use a lambda to retain original names\n",
    "# summary_stats.index = summary_stats.index.map(lambda x: utils.PSR_TYPE_MAPPING.get(x, x))\n",
    "\n",
    "# Reorder columns for better readability\n",
    "summary_stats = summary_stats[['Min', 'Min Date', 'Max', 'Max Date', 'Mean', 'Median']]\n",
    "\n",
    "# Adjust pandas display options to prevent wrapping and display all columns\n",
    "pd.set_option('display.max_columns', None)           # Display all columns\n",
    "pd.set_option('display.width', 1000)                 # Set a large display width\n",
    "pd.set_option('display.expand_frame_repr', False)    # Prevent wrapping to multiple lines\n",
    "\n",
    "# Option 1: Format percentage columns with '%' symbol by converting to strings\n",
    "summary_stats_formatted = summary_stats.copy()\n",
    "summary_stats_formatted[['Min', 'Max', 'Mean', 'Median']] = summary_stats_formatted[['Min', 'Max', 'Mean', 'Median']].applymap(lambda x: f\"{x:.2f}%\")\n",
    "\n",
    "# *** Enhanced Display Using Pandas Styler ***\n",
    "def highlight_min_max(s, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Highlight the min and max values in a series.\n",
    "    \"\"\"\n",
    "    return ['background-color: #FFDDC1' if v == min_val else \n",
    "            'background-color: #C1FFD7' if v == max_val else '' for v in s]\n",
    "\n",
    "# Create a Styler object\n",
    "styler = summary_stats.style\n",
    "\n",
    "# Apply highlighting to Min and Max columns\n",
    "for column in ['Min', 'Max']:\n",
    "    min_val = summary_stats[column].min()\n",
    "    max_val = summary_stats[column].max()\n",
    "    styler = styler.apply(lambda x: highlight_min_max(x, min_val, max_val), subset=[column])\n",
    "\n",
    "# Apply color gradients to percentage columns\n",
    "styler = styler.background_gradient(subset=['Min', 'Max', 'Mean', 'Median'], cmap='Blues')\n",
    "\n",
    "# Bold the header\n",
    "styler = styler.set_table_styles(\n",
    "    [{\n",
    "        'selector': 'th',\n",
    "        'props': [('font-size', '12pt'), ('text-align', 'center'), ('background-color', '#40466e'), ('color', 'white')]\n",
    "    },\n",
    "    {\n",
    "        'selector': 'td',\n",
    "        'props': [('padding', '5px'), ('text-align', 'center')]\n",
    "    }]\n",
    ")\n",
    "\n",
    "# *** NEW STEP: Ensure Date Columns Have White Background ***\n",
    "styler = styler.set_properties(\n",
    "    subset=['Min Date', 'Max Date'],\n",
    "    **{'background-color': 'white'}\n",
    ")\n",
    "styler = styler.set_properties(\n",
    "    subset=['Min Date', 'Max Date'],\n",
    "    **{'color': 'gray'}\n",
    ")\n",
    "\n",
    "# Format the date columns for better display\n",
    "styler = styler.format({\n",
    "    'Min Date': lambda v: v.strftime('%Y-%m-%d'),\n",
    "    'Max Date': lambda v: v.strftime('%Y-%m-%d'),\n",
    "    'Min': \"{:.2f}%\",\n",
    "    'Max': \"{:.2f}%\",\n",
    "    'Mean': \"{:.2f}%\",\n",
    "    'Median': \"{:.2f}%\"\n",
    "})\n",
    "\n",
    "# Define zebra striping using CSS nth-child selectors\n",
    "styler = styler.set_table_styles([\n",
    "    {\n",
    "        'selector': 'tbody tr:nth-child(odd)',\n",
    "        'props': [('background-color', 'steelblue')]  # Light Blue\n",
    "    },\n",
    "    {\n",
    "        'selector': 'tbody tr:nth-child(even)',\n",
    "        'props': [('background-color', '#236ca8')]  # Dark Blue\n",
    "    }\n",
    "])\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styler\n",
    "\n",
    "# *** Optional: Export to HTML ***\n",
    "# To save the styled DataFrame as an HTML file (useful for reports)\n",
    "# with open('summary_stats.html', 'w') as f:\n",
    "#     f.write(styler.render())\n",
    "\n",
    "# Reset pandas display options to their default values if needed\n",
    "# pd.reset_option('display.max_columns')\n",
    "# pd.reset_option('display.width')\n",
    "# pd.reset_option('display.expand_frame_repr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Sample data (same as above)\n",
    "\n",
    "# Plotting with Plotly\n",
    "fig = px.line(df_normalized, x=df_normalized.index, y='Nuclear', title='Share of Nuclear in Portuguese electricity mix')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_normalized['MA15d'] = df_normalized['Nuclear'].rolling(window=15*24, min_periods=1).mean() #min_periods=1 to avoid NaN at the start\n",
    "df_normalized['MA30d'] = df_normalized['Nuclear'].rolling(window=30*24, min_periods=1).mean() #min_periods=1 to avoid NaN at the start\n",
    "df_normalized['MA180d'] = df_normalized['Nuclear'].rolling(window=180*24, min_periods=1).mean() #min_periods=1 to avoid NaN at the start\n",
    "df_normalized['MA365d'] = df_normalized['Nuclear'].rolling(window=365*24, min_periods=1).mean() #min_periods=1 to avoid NaN at the start\n",
    "\n",
    "\n",
    "# Plotting with Plotly\n",
    "fig = px.line(df_normalized, x=df_normalized.index, y=['Nuclear', 'MA15d', 'MA180d', 'MA365d'], title='Share of Nuclear in Portuguese electricity mix')\n",
    "# fig.update_traces(mode='lines+markers') #Adding markers for better visualization\n",
    "# fig.update_layout(legend_title_text = 'Legend')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Add rolling averages\n",
    "df_normalized['MA15d'] = df_normalized['Nuclear'].rolling(window=15*24, min_periods=1).mean()  # 15 days\n",
    "df_normalized['MA30d'] = df_normalized['Nuclear'].rolling(window=30*24, min_periods=1).mean()  # 30 days\n",
    "df_normalized['MA180d'] = df_normalized['Nuclear'].rolling(window=180*24, min_periods=1).mean()  # 6 months\n",
    "df_normalized['MA365d'] = df_normalized['Nuclear'].rolling(window=365*24, min_periods=1).mean()  # 1 year\n",
    "\n",
    "# Plotting with Plotly and customized traces\n",
    "fig = px.line(title='Share of Nuclear in Portuguese electricity mix')\n",
    "\n",
    "# Add traces with distinct styles\n",
    "fig.add_scatter(x=df_normalized.index, y=df_normalized['Nuclear'], mode='lines',\n",
    "                line=dict(color='blue', width=1), name='Nuclear', opacity=0.4)\n",
    "fig.add_scatter(x=df_normalized.index, y=df_normalized['MA15d'], mode='lines',\n",
    "                line=dict(color='red', dash='solid', width=2), name='15-day MA')\n",
    "# fig.add_scatter(x=df_normalized.index, y=df_normalized['MA30d'], mode='lines',\n",
    "#                 line=dict(color='green', dash='dot', width=2), name='30-day MA')\n",
    "fig.add_scatter(x=df_normalized.index, y=df_normalized['MA180d'], mode='lines',\n",
    "                line=dict(color='orange', dash='solid', width=2), name='180-day MA')\n",
    "fig.add_scatter(x=df_normalized.index, y=df_normalized['MA365d'], mode='lines',\n",
    "                line=dict(color='purple', dash='solid', width=2), name='365-day MA')\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Percentage (%)\",\n",
    "    legend_title=\"Legend\",\n",
    "    template=\"plotly_white\",\n",
    "    title_x=0.5,  # Center the title\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create cumulative probability plot\n",
    "generation_pt_2 = generation_pt.copy()\n",
    "generation_pt_2['B19'].sort_values().reset_index(drop=True).reset_index().assign(\n",
    "    prob=lambda x: (x.index + 1)/len(x)\n",
    ").plot(x='B19', y='prob')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel('Power (MW)')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Portugal: Wind Power Cumulative Probability. P(Power >= x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "generation_pt = pd.read_pickle(\"../.data_cache/generation_pt.pkl.gz\")\n",
    "\n",
    "generation_pt_2['B19'] = generation_pt_2['B19'].interpolate()\n",
    "generation_pt_2['B19'].sort_values().reset_index(drop=True).reset_index().assign(\n",
    "    prob=lambda x: (1 - x.index/len(x))*100\n",
    ").plot(x='B19', y='prob', label='Exceedance Probability')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel('Power (MW)') \n",
    "plt.ylabel('P(X ≥ x)')\n",
    "plt.title('Portugal Wind Power Generation (Complementary Cumulative Distribution)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pt[\"B19\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calculate probabilities \n",
    "df = generation_pt_2['B19'].sort_values().reset_index(drop=True).reset_index().assign(\n",
    "    prob=lambda x: (1 - x.index/len(x))*100\n",
    ")\n",
    "\n",
    "# Create plotly figure\n",
    "fig = px.line(df, x='B19', y='prob')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Portugal Wind Power (Complementary Cumulative Distribution)',\n",
    "    xaxis_title='Power (MW)',\n",
    "    yaxis_title='P(X ≥ x)',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the Excel file without headers\n",
    "file_path = \"Potencias instaladas PT.xlsx\"  # Replace with your actual file path\n",
    "df = pd.read_excel(file_path, header=None)\n",
    "\n",
    "# Step 2: Assign custom column names\n",
    "df.columns = ['Month', 'Year', 'Eolica', 'Solar', 'Hidrica', 'Bombagem', 'Biomassa']\n",
    "\n",
    "# Step 3: Inspect the \"Month\" column\n",
    "print(\"Unique values in 'Month' column before cleaning:\")\n",
    "print(df['Month'].unique())\n",
    "\n",
    "# Step 4: Clean the \"Month\" column\n",
    "# - Strip leading/trailing spaces\n",
    "# - Capitalize the first letter (assuming month names are in Portuguese)\n",
    "df['Month'] = df['Month'].astype(str).str.strip().str.capitalize()\n",
    "\n",
    "# Verify cleaning\n",
    "print(\"\\nUnique values in 'Month' column after cleaning:\")\n",
    "print(df['Month'].unique())\n",
    "\n",
    "# Step 5: Map month names to numbers\n",
    "month_map = {\n",
    "    'Jan': 1, 'Fev': 2, 'Mar': 3, 'Abr': 4,\n",
    "    'Mai': 5, 'Jun': 6, 'Jul': 7, 'Ago': 8,\n",
    "    'Set': 9, 'Out': 10, 'Nov': 11, 'Dez': 12\n",
    "}\n",
    "\n",
    "df['Month_num'] = df['Month'].map(month_map)\n",
    "\n",
    "# Step 6: Identify any unmapped months\n",
    "unmapped_months = df[df['Month_num'].isna()]['Month'].unique()\n",
    "if len(unmapped_months) > 0:\n",
    "    print(\"\\nUnmapped months found:\", unmapped_months)\n",
    "    # Optionally, handle these unmapped months\n",
    "    # For example, you can manually add mappings or correct typos in your data\n",
    "\n",
    "# Step 7: Drop rows with unmapped months or handle them as desired\n",
    "df = df.dropna(subset=['Month_num'])  # This drops rows where month mapping failed\n",
    "\n",
    "# Convert 'Month_num' and 'Year' to integers (if they aren't already)\n",
    "df['Month_num'] = df['Month_num'].astype(int)\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# Step 8: Create the 'Datetime' column\n",
    "df['Datetime'] = pd.to_datetime(df[['Year', 'Month_num']].assign(Day=1), format='%Y-%m-%d')\n",
    "\n",
    "# Step 9: Set 'Datetime' as the index\n",
    "df.set_index('Datetime', inplace=True)\n",
    "\n",
    "# Step 10: Drop the original 'Year', 'Month', and 'Month_num' columns if no longer needed\n",
    "df.drop(columns=['Year', 'Month', 'Month_num'], inplace=True)\n",
    "\n",
    "# Step 11: Display the transformed DataFrame\n",
    "print(\"\\nTransformed DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Optionally, save the transformed DataFrame to a new Excel file\n",
    "# df.to_excel(\"transformed_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Máximo de importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribution_pt = contributions[\"PT\"]\n",
    "contribution_pt_total = contribution_pt.sum(axis=1, skipna=True)\n",
    "aggregated_total = aggregated.sum(axis=1, skipna=True)\n",
    "\n",
    "fraction_domestic = contribution_pt_total.div(aggregated_total)\n",
    "\n",
    "fraction_import = (1-fraction_domestic)\n",
    "\n",
    "fraction_import.idxmax(), fraction_import.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribution_es = contributions[\"ES\"]\n",
    "contribution_fr = contributions[\"FR\"]\n",
    "\n",
    "imports = contribution_es.add(contribution_fr, fill_value=0)\n",
    "imports_total = imports.sum(axis=1, skipna=True)\n",
    "\n",
    "imports_total.idxmax(), imports_total.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(contribution_pt.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribution_pt.loc[Timestamp('2024-06-09 07:00:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a specific timestamp but retain it in the context of the DataFrame\n",
    "contribution_pt.loc[['2024-06-09 07:00:00']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loads/diagrama de consumos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply nest_asyncio to allow nested event loops (useful for Jupyter notebooks)\n",
    "\n",
    "fetcher = ENTSOEDataFetcher()\n",
    "\n",
    "data_request = SimpleInterval(datetime(2024, 1,1,0), datetime(2025, 1,1,0))\n",
    "data = fetcher.get_data(data_request)\n",
    "consumption, contributions = analyzer.analyze(data)\n",
    "contribution_pt = contributions[\"PT\"]\n",
    "total_consumption = consumption.sum(axis=1)\n",
    "\n",
    "\n",
    "renewables = ['B01', 'B09', 'B10', 'B11', 'B12', 'B13', 'B15', 'B16', 'B18', 'B19']\n",
    "contribution_pt_renewables = contribution_pt[contribution_pt.columns.intersection(renewables)]\n",
    "total_contribution_pt_renewables = contribution_pt_renewables.sum(axis=1)\n",
    "\n",
    "generation_pt_interval = generation_pt.loc['2024-01-01 00:00:00':'2024-12-31 23:00:00']\n",
    "generation_pt_renewables = generation_pt_interval[generation_pt_interval.columns.intersection(renewables)]\n",
    "total_generation_pt_renewables = generation_pt_renewables.sum(axis=1)\n",
    "\n",
    "\n",
    "consumo_liquido = total_consumption-total_contribution_pt_renewables\n",
    "consumo_liquido_alt = total_consumption-total_generation_pt_renewables\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort the total generation values in descending order to create the duration curve\n",
    "sorted_consumption = total_consumption.sort_values(ascending=False)\n",
    "sorted_pt_generation_renewables = total_generation_pt_renewables.sort_values(ascending=False)\n",
    "sorted_pt_contribution_renewables = total_contribution_pt_renewables.sort_values(ascending=False)\n",
    "sorted_consumo_liquido = consumo_liquido.sort_values(ascending=False)\n",
    "sorted_consumo_liquido_alt = consumo_liquido_alt.sort_values(ascending=False)\n",
    "\n",
    "# Plot the duration curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sorted_consumption.values, label=\"Load\")\n",
    "plt.plot(sorted_pt_generation_renewables.values, label=\"Renewables Generation\")\n",
    "plt.plot(sorted_pt_contribution_renewables.values, label=\"Renewables Generation minus Exports\")\n",
    "plt.plot(sorted_consumo_liquido.values, label=\"Load minus Renewables (excluding renewables exports)\")\n",
    "plt.plot(sorted_consumo_liquido_alt.values, label=\"Load minus Renewable (including renewables exports)\")\n",
    "\n",
    "# Add a 1400 MW horizontal bar with solid red color and red fill\n",
    "plt.axhline(y=1400, color='red', linestyle='-', label=\"1400 MW\")\n",
    "plt.fill_between(range(len(sorted_consumo_liquido.values)), 0, np.minimum(1400, sorted_consumo_liquido), color='red', alpha=0.2)\n",
    "\n",
    "plt.title(\"Portugal Power Duration Curves\")\n",
    "plt.xlabel(\"Hours\")\n",
    "plt.ylabel(\"Power (MW)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of hours where \"Total Load minus Renewables\" is above 1400 MW\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the number of hours where the value is above 1400 MW\n",
    "hours_above_1400 = np.sum(sorted_consumo_liquido.values > 1400)\n",
    "\n",
    "# Print the result\n",
    "print(f\"The total load minus renewables is above 1400 MW for {hours_above_1400} hours in the graph.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import integrate\n",
    "# from scipy.integrate import simps\n",
    "# from scipy.integrate._quadrature import simps\n",
    "\n",
    "# Define the function to integrate: min(1400, total load minus renewables)\n",
    "bounded_values = np.maximum(0, np.minimum(1400, sorted_consumo_liquido.values))\n",
    "\n",
    "# Perform the integration using the composite Simpson's rule\n",
    "# Assuming each hour is represented as 1 unit in the x-axis\n",
    "hours = np.arange(len(bounded_values))  # x-axis values (0 to number of hours - 1)\n",
    "integrated_value = integrate.simpson(bounded_values)\n",
    "\n",
    "capacity_factor = integrated_value/(8766*1400)\n",
    "\n",
    "# Print the result\n",
    "print(f\"The integrated value of min(1400, total load minus renewables) over the year is {integrated_value:.2f} MW-hours.\")\n",
    "print(f\"Capacity factor: {capacity_factor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_consumo_liquido_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pt.loc['2024-12-29 00:00:00':'2025-01-01 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gener"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "electricity_consumption_mix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
