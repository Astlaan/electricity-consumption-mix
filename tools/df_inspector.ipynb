{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diogo\\Documents\\Code\\electricity_consumption_mix\\tools\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandasgui import show\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "flow_pt_to_es = pd.read_pickle(\"../.data_cache/flow_pt_to_es.pkl.gz\")\n",
    "flow_es_to_pt = pd.read_pickle(\"../.data_cache/flow_es_to_pt.pkl.gz\")\n",
    "generation_pt = pd.read_pickle(\"../.data_cache/generation_pt.pkl.gz\")\n",
    "generation_es = pd.read_pickle(\"../.data_cache/generation_es.pkl.gz\")\n",
    "\n",
    "# Show all dataframes using pandasgui\n",
    "show(flow_pt_to_es)\n",
    "show(flow_es_to_pt) \n",
    "show(generation_pt)\n",
    "show(generation_es)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change cache file format\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# flow_pt_to_es = pd.read_pickle(\"../.data_cache/flow_pt_to_es.pkl.gz\")\n",
    "# flow_es_to_pt = pd.read_pickle(\"../.data_cache/flow_es_to_pt.pkl.gz\")\n",
    "# generation_pt = pd.read_pickle(\"../.data_cache/generation_pt.pkl.gz\")\n",
    "# generation_es = pd.read_pickle(\"../.data_cache/generation_es.pkl.gz\")\n",
    "\n",
    "# # Save dataframes to pickle in .data_cache directory\n",
    "# flow_pt_to_es.to_pickle(\"../.data_cache/flow_pt_to_es.pkl.gz\", compression={'method': 'gzip', 'compresslevel': 1, \"mtime\": 0})\n",
    "# flow_es_to_pt.to_pickle(\"../.data_cache/flow_es_to_pt.pkl.gz\", compression={'method': 'gzip', 'compresslevel': 1, \"mtime\": 0}) \n",
    "# generation_pt.to_pickle(\"../.data_cache/generation_pt.pkl.gz\", compression={'method': 'gzip', 'compresslevel': 1, \"mtime\": 0})\n",
    "# generation_es.to_pickle(\"../.data_cache/generation_es.pkl.gz\", compression={'method': 'gzip', 'compresslevel': 1, \"mtime\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flow PT to ES:\n",
      "Start date min: 2015-01-10 00:00:00\n",
      "Start date max: 2024-10-29 23:00:00\n",
      "All hours present - continuous data\n",
      "\n",
      "Flow ES to PT:\n",
      "Start date min: 2015-01-10 00:00:00\n",
      "Start date max: 2024-10-29 23:00:00\n",
      "All hours present - continuous data\n",
      "\n",
      "Generation PT:\n",
      "Start date min: 2015-01-10 00:00:00\n",
      "Start date max: 2024-10-29 23:00:00\n",
      "All hours present - continuous data\n",
      "\n",
      "Generation ES:\n",
      "Start date min: 2015-01-10 00:00:00\n",
      "Start date max: 2024-10-29 23:00:00\n",
      "All hours present - continuous data\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def check_continuous_hours(df, name):\n",
    "    min_date = df['start_time'].min()\n",
    "    max_date = df['start_time'].max()\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Start date min: {min_date}\")\n",
    "    print(f\"Start date max: {max_date}\")\n",
    "    \n",
    "    # Generate expected datetime range with hourly frequency\n",
    "    expected_dates = pd.date_range(start=min_date, end=max_date, freq='H')\n",
    "    \n",
    "    # Convert df dates to set for efficient lookup\n",
    "    actual_dates = set(df['start_time'])\n",
    "    \n",
    "    # Find missing dates\n",
    "    missing_dates = set(expected_dates) - actual_dates\n",
    "    \n",
    "    if missing_dates:\n",
    "        print(f\"Missing {len(missing_dates)} hours:\")\n",
    "        for date in sorted(missing_dates)[:10]:  # Show first 10 missing dates\n",
    "            print(date)\n",
    "        if len(missing_dates) > 10:\n",
    "            print(\"...\")\n",
    "    else:\n",
    "        print(\"All hours present - continuous data\")\n",
    "\n",
    "# Check each dataframe\n",
    "check_continuous_hours(flow_pt_to_es, \"Flow PT to ES\")\n",
    "check_continuous_hours(flow_es_to_pt, \"Flow ES to PT\") \n",
    "check_continuous_hours(generation_pt, \"Generation PT\")\n",
    "check_continuous_hours(generation_es, \"Generation ES\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flow PT to ES:\n",
      "No NaN values found\n",
      "\n",
      "Flow ES to PT:\n",
      "No NaN values found\n",
      "\n",
      "Generation PT:\n",
      "Total NaN values: 47254\n",
      "\n",
      "NaN counts by column:\n",
      "B18: 47254\n",
      "\n",
      "Generation ES:\n",
      "Total NaN values: 69867\n",
      "\n",
      "NaN counts by column:\n",
      "B01: 23\n",
      "B02: 23\n",
      "B03: 23\n",
      "B04: 23\n",
      "B05: 23\n",
      "B06: 23\n",
      "B07: 23\n",
      "B08: 23\n",
      "B09: 23\n",
      "B11: 23\n",
      "B12: 23\n",
      "B13: 24\n",
      "B14: 23\n",
      "B15: 23\n",
      "B16: 23\n",
      "B17: 23\n",
      "B18: 23\n",
      "B19: 23\n",
      "B20: 23\n",
      "B10: 69429\n"
     ]
    }
   ],
   "source": [
    "def check_nans(df, name):\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    # Get total number of NaN values\n",
    "    total_nans = df.isna().sum().sum()\n",
    "    \n",
    "    if total_nans > 0:\n",
    "        print(f\"Total NaN values: {total_nans}\")\n",
    "        # Show NaN counts by column\n",
    "        nan_counts = df.isna().sum()\n",
    "        nan_columns = nan_counts[nan_counts > 0]\n",
    "        print(\"\\nNaN counts by column:\")\n",
    "        for col, count in nan_columns.items():\n",
    "            print(f\"{col}: {count}\")\n",
    "    else:\n",
    "        print(\"No NaN values found\")\n",
    "\n",
    "# Check each dataframe\n",
    "check_nans(flow_pt_to_es, \"Flow PT to ES\")\n",
    "check_nans(flow_es_to_pt, \"Flow ES to PT\")\n",
    "check_nans(generation_pt, \"Generation PT\") \n",
    "check_nans(generation_es, \"Generation ES\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flow PT to ES:\n",
      "No NaN values found\n",
      "\n",
      "Flow ES to PT:\n",
      "No NaN values found\n",
      "\n",
      "Generation PT:\n",
      "\n",
      "Example dates with NaN values:\n",
      "\n",
      "B18:\n",
      "2015-01-10 00:00:00\n",
      "2015-01-10 01:00:00\n",
      "2015-01-10 02:00:00\n",
      "2015-01-10 03:00:00\n",
      "2015-01-10 04:00:00\n",
      "\n",
      "Generation ES:\n",
      "\n",
      "Example dates with NaN values:\n",
      "\n",
      "B01:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B02:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B03:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B04:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B05:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B06:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B07:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B08:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B09:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B11:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B12:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B13:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B14:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B15:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B16:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B17:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B18:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B19:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B20:\n",
      "2015-01-19 18:00:00\n",
      "2015-01-19 19:00:00\n",
      "2015-01-27 18:00:00\n",
      "2015-01-28 12:00:00\n",
      "2015-02-10 10:00:00\n",
      "\n",
      "B10:\n",
      "2015-01-10 00:00:00\n",
      "2015-01-10 01:00:00\n",
      "2015-01-10 02:00:00\n",
      "2015-01-10 03:00:00\n",
      "2015-01-10 04:00:00\n"
     ]
    }
   ],
   "source": [
    "def show_nan_examples(df, name, n=5):\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    # Get columns with NaN values\n",
    "    nan_columns = df.columns[df.isna().any()].tolist()\n",
    "    \n",
    "    if not nan_columns:\n",
    "        print(\"No NaN values found\")\n",
    "        return\n",
    "        \n",
    "    print(\"\\nExample dates with NaN values:\")\n",
    "    for col in nan_columns:\n",
    "        nan_dates = df[df[col].isna()]['start_time'].head(n)\n",
    "        if not nan_dates.empty:\n",
    "            print(f\"\\n{col}:\")\n",
    "            for date in nan_dates:\n",
    "                print(date)\n",
    "\n",
    "# Check each dataframe                \n",
    "show_nan_examples(flow_pt_to_es, \"Flow PT to ES\")\n",
    "show_nan_examples(flow_es_to_pt, \"Flow ES to PT\")\n",
    "show_nan_examples(generation_pt, \"Generation PT\")\n",
    "show_nan_examples(generation_es, \"Generation ES\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if just one column without data -> it's fine\n",
    "if whole hour without data -> discard hour\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
