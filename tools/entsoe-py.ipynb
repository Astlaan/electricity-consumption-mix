{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entsoe import EntsoePandasClient\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.max_rows\", None)  # Show all rows\n",
    "pd.set_option(\"display.width\", None)  # Set width to expand to full display\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "client = EntsoePandasClient(api_key=os.getenv(\"ENTSOE_API_KEY\"))\n",
    "\n",
    "start = pd.Timestamp(\"2024-11-10T17:00\", tz=\"UTC\")\n",
    "end = pd.Timestamp(\"2024-11-10T18:00\", tz=\"UTC\")\n",
    "\n",
    "# df = client.query_crossborder_flows(\"ES\", \"PT\", start=start, end=end)\n",
    "df = client.query_generation(\"PT\", start=start, end=end, psr_type=None)\n",
    "df = df.tz_convert(\"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.resample('h').mean()\n",
    "df = df.xs(\"Actual Aggregated\", axis=1, level=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_fetcher import ENTSOEDataFetcher\n",
    "import nest_asyncio\n",
    "from data_fetcher import SimpleInterval\n",
    "from datetime import datetime\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "fetcher = ENTSOEDataFetcher()\n",
    "data_request = SimpleInterval(datetime(2018, 1,1,0), datetime(2024, 12,21,0))\n",
    "fetcher_data = fetcher.get_data(data_request)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_URL = \"https://web-api.tp.entsoe.eu/api\"\n",
    "\n",
    "params = {\n",
    "    \"documentType\": \"A73\",\n",
    "    \"processType\": \"A16\",\n",
    "    \"in_Domain\": \"10YFI-1--------U\",\n",
    "    \"periodStart\": \"202401010000\",\n",
    "    \"periodEnd\": \"202401020000\",\n",
    "    \"psrType\": \"B14\",\n",
    "    \"securityToken\": os.getenv(\"ENTSOE_API_KEY\")\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from aiohttp import ClientSession, TCPConnector\n",
    "from datetime import datetime, timedelta\n",
    "from aiolimiter import AsyncLimiter\n",
    "\n",
    "BASE_URL = \"https://web-api.tp.entsoe.eu/api\"\n",
    "\n",
    "params = {\n",
    "    \"documentType\": \"A73\",\n",
    "    \"processType\": \"A16\",\n",
    "    \"in_Domain\": \"10YFI-1--------U\",\n",
    "    \"periodStart\": \"202401010000\",\n",
    "    \"periodEnd\": \"202401020000\",\n",
    "    \"psrType\": \"B14\",\n",
    "    \"securityToken\": os.getenv(\"ENTSOE_API_KEY\")\n",
    "}\n",
    "\n",
    "# Define date range\n",
    "START_DATE = datetime(2024, 1, 1)\n",
    "END_DATE = datetime(2024, 12, 27)\n",
    "\n",
    "# Define rate limiter\n",
    "rate_limiter = AsyncLimiter(max_rate=200, time_period=60)  # 200 requests per 60 seconds\n",
    "\n",
    "# Define asynchronous request function\n",
    "async def fetch(session: ClientSession, params: dict):\n",
    "    async with rate_limiter:\n",
    "        async with session.get(BASE_URL, params=params) as response:\n",
    "            response.raise_for_status()\n",
    "            return await response.text()\n",
    "\n",
    "# Define main asynchronous function\n",
    "async def main():\n",
    "    connector = TCPConnector(limit=7)  # Max 7 concurrent connections\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        tasks = []\n",
    "        current_date = START_DATE\n",
    "        while current_date <= END_DATE:\n",
    "            params['periodStart'] = current_date.strftime('%Y%m%d0000')\n",
    "            params['periodEnd'] = (current_date + timedelta(days=1)).strftime('%Y%m%d0000')\n",
    "            task = asyncio.create_task(fetch(session, params.copy()))\n",
    "            tasks.append(task)\n",
    "            current_date += timedelta(days=1)\n",
    "        \n",
    "        responses = []\n",
    "        for i, task in enumerate(asyncio.as_completed(tasks), 1):\n",
    "            try:\n",
    "                response = await task\n",
    "                responses.append(response)\n",
    "                if i % 50 == 0:\n",
    "                    print(f\"Processed {i} requests\")\n",
    "            except Exception as e:\n",
    "                print(f\"Request {i} failed: {e}\")\n",
    "    return responses\n",
    "        # Process responses as needed\n",
    "        # For example, save to a file or parse the JSON\n",
    "        # with open('responses.json', 'w') as f:\n",
    "        #     json.dump(responses, f)\n",
    "\n",
    "# Run the asynchronous main function\n",
    "responses = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Sample XML data (Replace the ellipsis with your actual XML content)\n",
    "def parse_xml(xml_data):\n",
    "    # Define the namespace\n",
    "    namespaces = {\n",
    "        'ns': 'urn:iec62325.351:tc57wg16:451-6:generationloaddocument:3:0'\n",
    "    }\n",
    "\n",
    "    # Initialize a list to store the extracted data\n",
    "    data = []\n",
    "\n",
    "    # Parse the XML data\n",
    "    root = ET.fromstring(xml_data)\n",
    "\n",
    "    # Iterate through each TimeSeries element\n",
    "    for ts in root.findall('ns:TimeSeries', namespaces):\n",
    "        # Extract the generating unit name\n",
    "        unit = ts.find('.//ns:PowerSystemResources/ns:name', namespaces)\n",
    "        if unit is not None:\n",
    "            unit_name = unit.text\n",
    "        else:\n",
    "            unit_name = None  # Handle cases where the name might be missing\n",
    "\n",
    "        # Extract the time interval\n",
    "        start_time_str = ts.find('.//ns:timeInterval/ns:start', namespaces)\n",
    "        end_time_str = ts.find('.//ns:timeInterval/ns:end', namespaces)\n",
    "        \n",
    "        if start_time_str is not None and end_time_str is not None:\n",
    "            start_time = datetime.fromisoformat(start_time_str.text.replace('Z', '+00:00'))\n",
    "            end_time = datetime.fromisoformat(end_time_str.text.replace('Z', '+00:00'))\n",
    "        else:\n",
    "            continue  # Skip this TimeSeries if time interval is missing\n",
    "\n",
    "        # Extract the resolution\n",
    "        resolution_str = ts.find('.//ns:resolution', namespaces)\n",
    "        if resolution_str is not None:\n",
    "            resolution_iso = resolution_str.text\n",
    "            # Assuming the resolution is in the format PT{minutes}M\n",
    "            resolution_minutes = int(resolution_iso.replace('PT', '').replace('M', ''))\n",
    "        else:\n",
    "            resolution_minutes = 60  # Default to 60 minutes if missing\n",
    "\n",
    "        # Iterate through each Point within the Period\n",
    "        for point in ts.findall('.//ns:Point', namespaces):\n",
    "            position = point.find('ns:position', namespaces)\n",
    "            quantity = point.find('ns:quantity', namespaces)\n",
    "            \n",
    "            if position is not None and quantity is not None:\n",
    "                try:\n",
    "                    pos = int(position.text)\n",
    "                    qty = float(quantity.text)\n",
    "                except ValueError:\n",
    "                    continue  # Skip if conversion fails\n",
    "\n",
    "                # Calculate the timestamp\n",
    "                timestamp = start_time + timedelta(minutes=resolution_minutes * (pos - 1))\n",
    "                \n",
    "                # Append the extracted information to the data list\n",
    "                data.append({\n",
    "                    'datetime': timestamp,\n",
    "                    'value': qty,\n",
    "                    'generating_unit': unit_name\n",
    "                })\n",
    "    return data\n",
    "# Create the pandas DataFrame\n",
    "\n",
    "parsed_data = list(map(parse_xml, responses))\n",
    "df = pd.DataFrame([item for sublist in parsed_data for item in sublist])\n",
    "df.set_index('datetime', inplace=True)\n",
    "df = df.pivot(columns='generating_unit', values='value')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olkiluoto = df.filter(like='Olkiluoto', axis=1)\n",
    "df_olkiluoto.sum(axis=1).sum()/1000/1000 # TWh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_since_start_of_year = (datetime(2024, 12, 23) - datetime(2024, 1, 1)).days + 1\n",
    "days_since_start_of_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df_olkiluoto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finland imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_fetcher import ENTSOEDataFetcher\n",
    "import nest_asyncio\n",
    "from data_fetcher import SimpleInterval\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "from aiolimiter import AsyncLimiter  # To manage rate limiting\n",
    "\n",
    "data_fetcher = ENTSOEDataFetcher()\n",
    "\n",
    "finland_eic = \"10YFI-1--------U\"\n",
    "neighbors = {\"NO\": \"10YNO-0--------C\", \"SE\": \"10YSE-1--------K\", \"RU\": \"RU\", \"EE\": \"10Y1001A1001A39I\"}\n",
    "\n",
    "# Set up concurrency and rate limiting:\n",
    "# Limit to a maximum of 5 concurrent requests\n",
    "semaphore = asyncio.Semaphore(5)\n",
    "# Limit to a maximum of 100 requests per minute\n",
    "rate_limiter = AsyncLimiter(100, 60)\n",
    "\n",
    "async def limited_get_physical_flows(*args, **kwargs):\n",
    "    # Wrapper function to enforce both concurrency and rate limits\n",
    "    async with semaphore:\n",
    "        async with rate_limiter:\n",
    "            return await data_fetcher._async_get_physical_flows(*args, **kwargs)\n",
    "\n",
    "async def get_flows():\n",
    "    # Define the time range for the flows\n",
    "    start_date = datetime(2015, 2, 1)\n",
    "    end_date = datetime(2025, 2, 12)\n",
    "    flows = {\"imports\": {}, \"exports\": {}}\n",
    "    tasks = []\n",
    "    \n",
    "    # Create tasks for all neighbors concurrently\n",
    "    for neighbor_label, neighbor_domain in tqdm(neighbors.items(), desc=\"Processing neighbors\", unit=\"neighbor\"):\n",
    "        import_task = asyncio.create_task(\n",
    "            limited_get_physical_flows(\n",
    "                out_domain=neighbor_domain,\n",
    "                in_domain=finland_eic,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date\n",
    "            )\n",
    "        )\n",
    "        export_task = asyncio.create_task(\n",
    "            limited_get_physical_flows(\n",
    "                out_domain=finland_eic,\n",
    "                in_domain=neighbor_domain,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date\n",
    "            )\n",
    "        )\n",
    "        tasks.append((neighbor_label, import_task, export_task))\n",
    "    \n",
    "    # Await all tasks concurrently and gather results\n",
    "    for neighbor_label, import_task, export_task in tasks:\n",
    "        flows[\"imports\"][neighbor_label], flows[\"exports\"][neighbor_label] = await asyncio.gather(import_task, export_task)\n",
    "    \n",
    "    return flows\n",
    "\n",
    "# Run the async function using the current event loop (nest_asyncio is already imported)\n",
    "nest_asyncio.apply()\n",
    "flows_result = asyncio.get_event_loop().run_until_complete(get_flows())\n",
    "print(flows_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before concatenating, ensure each dataframe has its timestamp column set as the index\n",
    "imports_dfs = [df.set_index('start_time') for df in flows_result[\"imports\"].values()]\n",
    "exports_dfs = [df.set_index('start_time') for df in flows_result[\"exports\"].values()]\n",
    "\n",
    "# Combine all the imports dataframes by concatenating along columns (aligning on index) and summing row-wise.\n",
    "aggregated_imports = pd.concat([df[\"Power\"] for df in imports_dfs], axis=1).sum(axis=1)\n",
    "aggregated_exports = pd.concat([df[\"Power\"] for df in exports_dfs], axis=1).sum(axis=1)\n",
    "\n",
    "# Optionally, sort the resulting dataframes by their index (timestamp) for clarity.\n",
    "aggregated_imports = aggregated_imports.sort_index()\n",
    "aggregated_exports = aggregated_exports.sort_index()\n",
    "\n",
    "# Define the final variables\n",
    "imports = aggregated_imports\n",
    "exports = aggregated_exports\n",
    "\n",
    "# For debugging, print a preview of the aggregated data\n",
    "print(\"Aggregated Imports Data:\")\n",
    "print(imports.head())\n",
    "print(\"\\nAggregated Exports Data:\")\n",
    "print(exports.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_imports = imports - exports\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate smoothed mean and standard deviation using a 1-month rolling window\n",
    "smoothed_mean = net_imports.rolling(window='30D').mean()\n",
    "smoothed_std = net_imports.rolling(window='30D').std()\n",
    "\n",
    "# Plot the smoothed mean along with standard deviation bands\n",
    "plt.plot(net_imports.index, smoothed_mean, color='blue', linewidth=2, label='30 Day SMA')\n",
    "plt.fill_between(net_imports.index, smoothed_mean - smoothed_std, smoothed_mean + smoothed_std, color='blue', alpha=0.2, label='±1 Std Dev')\n",
    "\n",
    "# Add a vertical line at Sunday, 16 April 2023 with a label\n",
    "event_date = datetime.datetime(2023, 4, 16)\n",
    "plt.axvline(x=event_date, color='green', linestyle='--', linewidth=2)\n",
    "ymax = plt.gca().get_ylim()[1]\n",
    "plt.text(event_date, ymax * 0.95, 'Olkiluoto-3 commercial operation start',\n",
    "         rotation=90, verticalalignment='top', color='green', fontsize=14)\n",
    "\n",
    "# Add another vertical line on the day of the Ukraine invasion (24 February 2022)\n",
    "invasion_date = datetime.datetime(2022, 2, 24)\n",
    "plt.axvline(x=invasion_date, color='orange', linestyle='--', linewidth=2)\n",
    "plt.text(invasion_date, ymax * 0.95, 'Russian invasion of Ukraine',\n",
    "         rotation=90, verticalalignment='top', color='orange', fontsize=14)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Net Imports')\n",
    "plt.title('Finland Electricity Net Imports')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_imports = imports - exports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Resample net_imports from hourly to daily frequency (using the mean)\n",
    "net_imports_daily = net_imports.resample('D').mean()\n",
    "\n",
    "# Perform seasonal decomposition assuming an additive model with an annual period\n",
    "decomposition = seasonal_decompose(net_imports_daily, model='additive', period=365)\n",
    "\n",
    "# Subtract the seasonal component to obtain a deseasonalized series\n",
    "deseasonalized = net_imports_daily - decomposition.seasonal\n",
    "\n",
    "# Smooth the deseasonalized series using a 30-day rolling window\n",
    "smoothed_mean = deseasonalized.rolling(window=30).mean()\n",
    "smoothed_std = deseasonalized.rolling(window=30).std()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot the smoothed deseasonalized net imports along with ±1 standard deviation bands\n",
    "plt.plot(deseasonalized.index, smoothed_mean, color='blue', linewidth=2, label='30 Day SMA (Deseasonalized)')\n",
    "plt.fill_between(deseasonalized.index, smoothed_mean - smoothed_std, smoothed_mean + smoothed_std, \n",
    "                 color='blue', alpha=0.2, label='±1 Std Dev')\n",
    "\n",
    "# Add a vertical line at Olkiluoto-3 commercial operation start (Sunday, 16 April 2023)\n",
    "event_date = datetime.datetime(2023, 4, 16)\n",
    "plt.axvline(x=event_date, color='green', linestyle='--', linewidth=2)\n",
    "ymax = plt.gca().get_ylim()[1]\n",
    "plt.text(event_date, ymax * 0.95, 'Olkiluoto-3 commercial operation start',\n",
    "         rotation=90, verticalalignment='top', color='green', fontsize=14)\n",
    "\n",
    "# Add a vertical line on the day of the Ukraine invasion (24 February 2022)\n",
    "invasion_date = datetime.datetime(2022, 2, 24)\n",
    "plt.axvline(x=invasion_date, color='orange', linestyle='--', linewidth=2)\n",
    "plt.text(invasion_date, ymax * 0.95, 'Russian invasion of Ukraine',\n",
    "         rotation=90, verticalalignment='top', color='orange', fontsize=14)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Deseasonalized Net Imports')\n",
    "plt.title('Finland Electricity Net Imports (Deseasonalized)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
